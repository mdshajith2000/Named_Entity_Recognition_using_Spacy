{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "io2AaJLHa7ym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b81d8d-9d04-4450-9544-5db098758259"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJc-edqsba-B"
      },
      "source": [
        "Let's take a sentence from an article published in the New york Times as the test data.\n",
        "Test data = 'United States tech giant Google on Wednesday launched a new cloud data hub in Warsaw its first in Central and Eastern Europe with an investment of nearly $2.0 billion i.e. €1.7 billion'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-UsY1GEbZp9"
      },
      "source": [
        "data = 'India reported 36,571 new Covid cases and 540 deaths in last 24 hours, according to health ministry bulletin on Friday. Active caseload stands at 3,63,605; lowest in 150 days. Meanwhile, the recovery rate has increased to 97.54%. Stay with TOI for all updates'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b16wZnC4bn_7"
      },
      "source": [
        "Then let's apply word tokenization and part-of-speech tagging to the data we have taken.\n",
        "Below is the function module which is used for the preprocessing of the data take."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4kgluN-bhVh"
      },
      "source": [
        "def preprocess(sent):\n",
        "    sent = nltk.word_tokenize(sent)\n",
        "    sent = nltk.pos_tag(sent)\n",
        "    return sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqyeLJsxeziW"
      },
      "source": [
        "By passing our test data as the parameter to out preprocess() function module, it processes the data and returns a list of tuples containing the words and their parts of speech."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A70L2Ycb6LD",
        "outputId": "e47e0c4f-b69e-48de-982d-e29388dd1342"
      },
      "source": [
        "sent = preprocess(data)\n",
        "sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('India', 'NNP'),\n",
              " ('reported', 'VBD'),\n",
              " ('36,571', 'CD'),\n",
              " ('new', 'JJ'),\n",
              " ('Covid', 'NNP'),\n",
              " ('cases', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('540', 'CD'),\n",
              " ('deaths', 'NNS'),\n",
              " ('in', 'IN'),\n",
              " ('last', 'JJ'),\n",
              " ('24', 'CD'),\n",
              " ('hours', 'NNS'),\n",
              " (',', ','),\n",
              " ('according', 'VBG'),\n",
              " ('to', 'TO'),\n",
              " ('health', 'NN'),\n",
              " ('ministry', 'NN'),\n",
              " ('bulletin', 'NN'),\n",
              " ('on', 'IN'),\n",
              " ('Friday', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Active', 'NNP'),\n",
              " ('caseload', 'NN'),\n",
              " ('stands', 'VBZ'),\n",
              " ('at', 'IN'),\n",
              " ('3,63,605', 'CD'),\n",
              " (';', ':'),\n",
              " ('lowest', 'JJS'),\n",
              " ('in', 'IN'),\n",
              " ('150', 'CD'),\n",
              " ('days', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('Meanwhile', 'RB'),\n",
              " (',', ','),\n",
              " ('the', 'DT'),\n",
              " ('recovery', 'NN'),\n",
              " ('rate', 'NN'),\n",
              " ('has', 'VBZ'),\n",
              " ('increased', 'VBN'),\n",
              " ('to', 'TO'),\n",
              " ('97.54', 'CD'),\n",
              " ('%', 'NN'),\n",
              " ('.', '.'),\n",
              " ('Stay', 'NNP'),\n",
              " ('with', 'IN'),\n",
              " ('TOI', 'NNP'),\n",
              " ('for', 'IN'),\n",
              " ('all', 'DT'),\n",
              " ('updates', 'NNS')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoOm56YRfPti"
      },
      "source": [
        "Our next process is chunking. Chunking is a process pf creating a chunk pattern that consists of one rule, that a noun phrase, NP, should be formed whenever the chunker finds an optional determiner, DT, followed by any number of adjectives, JJ, and then a noun, NN. Below is the pattern we form with the rule we discussed about. Chunks are formed by words and the kinds of words which are defined using the part-of-speech tags. A pattern can be defined as words that can’t be a part of chunks and such words are known as chinks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJYR389bb-dG"
      },
      "source": [
        "pattern = 'NP: {<DT>?<JJ>*<NN>}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKICXqIXkYro"
      },
      "source": [
        "The next step is to create a chunk parser and to test it on our test data. The output can be observed as a tree or a hierarchy with S as the root level which denotes the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WayYCJJZfjmA",
        "outputId": "023e5851-d565-4efe-f2a7-e09dfe84fe47"
      },
      "source": [
        "chun_p = nltk.RegexpParser(pattern)\n",
        "chun_sol = chun_p.parse(sent)\n",
        "print(chun_sol)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  India/NNP\n",
            "  reported/VBD\n",
            "  36,571/CD\n",
            "  new/JJ\n",
            "  Covid/NNP\n",
            "  cases/NNS\n",
            "  and/CC\n",
            "  540/CD\n",
            "  deaths/NNS\n",
            "  in/IN\n",
            "  last/JJ\n",
            "  24/CD\n",
            "  hours/NNS\n",
            "  ,/,\n",
            "  according/VBG\n",
            "  to/TO\n",
            "  (NP health/NN)\n",
            "  (NP ministry/NN)\n",
            "  (NP bulletin/NN)\n",
            "  on/IN\n",
            "  Friday/NNP\n",
            "  ./.\n",
            "  Active/NNP\n",
            "  (NP caseload/NN)\n",
            "  stands/VBZ\n",
            "  at/IN\n",
            "  3,63,605/CD\n",
            "  ;/:\n",
            "  lowest/JJS\n",
            "  in/IN\n",
            "  150/CD\n",
            "  days/NNS\n",
            "  ./.\n",
            "  Meanwhile/RB\n",
            "  ,/,\n",
            "  (NP the/DT recovery/NN)\n",
            "  (NP rate/NN)\n",
            "  has/VBZ\n",
            "  increased/VBN\n",
            "  to/TO\n",
            "  97.54/CD\n",
            "  (NP %/NN)\n",
            "  ./.\n",
            "  Stay/NNP\n",
            "  with/IN\n",
            "  TOI/NNP\n",
            "  for/IN\n",
            "  all/DT\n",
            "  updates/NNS)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G7b9diPu4i1"
      },
      "source": [
        "Before moving to the next part of the code, let's discuss about IOP tags. It is a chunks format. These tags are similar to part-of-speech tags but provided they can denote the inside, outside, and the beginning of a chunk. Not just noun phrase but multiple different chunk phrase types are allowed here. IOB tags have become the standard way to represent chunk structures in files, and we will also be using this format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzloZ0uikp5p",
        "outputId": "fa180929-10c3-491c-b4cf-5f24e55feb9f"
      },
      "source": [
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from pprint import pprint\n",
        "iob_tagged = tree2conlltags(chun_sol)\n",
        "pprint(iob_tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('India', 'NNP', 'O'),\n",
            " ('reported', 'VBD', 'O'),\n",
            " ('36,571', 'CD', 'O'),\n",
            " ('new', 'JJ', 'O'),\n",
            " ('Covid', 'NNP', 'O'),\n",
            " ('cases', 'NNS', 'O'),\n",
            " ('and', 'CC', 'O'),\n",
            " ('540', 'CD', 'O'),\n",
            " ('deaths', 'NNS', 'O'),\n",
            " ('in', 'IN', 'O'),\n",
            " ('last', 'JJ', 'O'),\n",
            " ('24', 'CD', 'O'),\n",
            " ('hours', 'NNS', 'O'),\n",
            " (',', ',', 'O'),\n",
            " ('according', 'VBG', 'O'),\n",
            " ('to', 'TO', 'O'),\n",
            " ('health', 'NN', 'B-NP'),\n",
            " ('ministry', 'NN', 'B-NP'),\n",
            " ('bulletin', 'NN', 'B-NP'),\n",
            " ('on', 'IN', 'O'),\n",
            " ('Friday', 'NNP', 'O'),\n",
            " ('.', '.', 'O'),\n",
            " ('Active', 'NNP', 'O'),\n",
            " ('caseload', 'NN', 'B-NP'),\n",
            " ('stands', 'VBZ', 'O'),\n",
            " ('at', 'IN', 'O'),\n",
            " ('3,63,605', 'CD', 'O'),\n",
            " (';', ':', 'O'),\n",
            " ('lowest', 'JJS', 'O'),\n",
            " ('in', 'IN', 'O'),\n",
            " ('150', 'CD', 'O'),\n",
            " ('days', 'NNS', 'O'),\n",
            " ('.', '.', 'O'),\n",
            " ('Meanwhile', 'RB', 'O'),\n",
            " (',', ',', 'O'),\n",
            " ('the', 'DT', 'B-NP'),\n",
            " ('recovery', 'NN', 'I-NP'),\n",
            " ('rate', 'NN', 'B-NP'),\n",
            " ('has', 'VBZ', 'O'),\n",
            " ('increased', 'VBN', 'O'),\n",
            " ('to', 'TO', 'O'),\n",
            " ('97.54', 'CD', 'O'),\n",
            " ('%', 'NN', 'B-NP'),\n",
            " ('.', '.', 'O'),\n",
            " ('Stay', 'NNP', 'O'),\n",
            " ('with', 'IN', 'O'),\n",
            " ('TOI', 'NNP', 'O'),\n",
            " ('for', 'IN', 'O'),\n",
            " ('all', 'DT', 'O'),\n",
            " ('updates', 'NNS', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08dtcnrZxPpr",
        "outputId": "56e0dc7a-10f1-4ca0-d77e-4dc37b2a72d5"
      },
      "source": [
        "ne_tree = nltk.ne_chunk(pos_tag(word_tokenize(data)))\n",
        "print(ne_tree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (GPE India/NNP)\n",
            "  reported/VBD\n",
            "  36,571/CD\n",
            "  new/JJ\n",
            "  Covid/NNP\n",
            "  cases/NNS\n",
            "  and/CC\n",
            "  540/CD\n",
            "  deaths/NNS\n",
            "  in/IN\n",
            "  last/JJ\n",
            "  24/CD\n",
            "  hours/NNS\n",
            "  ,/,\n",
            "  according/VBG\n",
            "  to/TO\n",
            "  health/NN\n",
            "  ministry/NN\n",
            "  bulletin/NN\n",
            "  on/IN\n",
            "  Friday/NNP\n",
            "  ./.\n",
            "  Active/NNP\n",
            "  caseload/NN\n",
            "  stands/VBZ\n",
            "  at/IN\n",
            "  3,63,605/CD\n",
            "  ;/:\n",
            "  lowest/JJS\n",
            "  in/IN\n",
            "  150/CD\n",
            "  days/NNS\n",
            "  ./.\n",
            "  Meanwhile/RB\n",
            "  ,/,\n",
            "  the/DT\n",
            "  recovery/NN\n",
            "  rate/NN\n",
            "  has/VBZ\n",
            "  increased/VBN\n",
            "  to/TO\n",
            "  97.54/CD\n",
            "  %/NN\n",
            "  ./.\n",
            "  Stay/NNP\n",
            "  with/IN\n",
            "  (ORGANIZATION TOI/NNP)\n",
            "  for/IN\n",
            "  all/DT\n",
            "  updates/NNS)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFtOHm-zbaLV"
      },
      "source": [
        "So far we have tried to perform the named entity recognition by using the nltk kit. It is observed that Google is idenitified as a person which is a disappointing output. Thus, we are going to use the named entity recognition module from Spacy which has been trained on the OntoNotes5 corpus and it can support recognition of various entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZSAuFVH8D3K"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAtIy-0ZcFA9"
      },
      "source": [
        "The same input which we used for the previous segment using ntlk kit was repeated here. This time we are using the Spacy's NER module. For the given data it idenitifies and tags the words in the sentence which are recogonised. Majorly consisting of the noun phrases. The meanings for the identified tags are given below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjzGlDqT9JVj",
        "outputId": "a715be94-8b6c-4c77-ba0d-377c510ed8a3"
      },
      "source": [
        "doc = nlp('India reported 36,571 new Covid cases and 540 deaths in last 24 hours, according to health ministry bulletin on Friday. Active caseload stands at 3,63,605; lowest in 150 days. Meanwhile, the recovery rate has increased to 97.54%. Stay with TOI for all updates')\n",
        "pprint([(X.text, X.label_) for X in doc.ents])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('India', 'GPE'),\n",
            " ('36,571', 'CARDINAL'),\n",
            " ('Covid', 'PRODUCT'),\n",
            " ('540', 'CARDINAL'),\n",
            " ('last 24 hours', 'TIME'),\n",
            " ('Friday', 'DATE'),\n",
            " ('3,63,605', 'CARDINAL'),\n",
            " ('150 days', 'DATE'),\n",
            " ('97.54%', 'PERCENT')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1cr8OnMc116"
      },
      "source": [
        "GPE : Countires, Syayes and Cities etc..\n",
        "NORP : Nationalities or religious or political groups\n",
        "ORG : Organization\n",
        "MONEY : Currency and Monetary values\n",
        "DATE : Relative or absolute date\n",
        "LOC : Non GPE locations\n",
        "ORDINAL : first, second etc.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLlDM0sIdVX-"
      },
      "source": [
        "In the previous segment we have identified the entities and tagged the recognised entities. Now let's explore the token level recognition which will tokenize our data and will let us explore the token-level entity annotation. It will use the BILUO tagging scheme. \n",
        "B : BEGIN (The first token of a multi-token entity)\n",
        "I : IN (An inner token of a multi-token entity)\n",
        "L : LAST (The last token of a multi-token entity)\n",
        "U : UNIT (A single token entity which occurs only once in the data)\n",
        "O : OUT (A non-entity token which means there doesn't exist such entity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpXFEwnL9NBp",
        "outputId": "c534818e-55e4-41b9-dc36-c91b34f8c242"
      },
      "source": [
        "pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(India, 'B', 'GPE'),\n",
            " (reported, 'O', ''),\n",
            " (36,571, 'B', 'CARDINAL'),\n",
            " (new, 'O', ''),\n",
            " (Covid, 'B', 'PRODUCT'),\n",
            " (cases, 'O', ''),\n",
            " (and, 'O', ''),\n",
            " (540, 'B', 'CARDINAL'),\n",
            " (deaths, 'O', ''),\n",
            " (in, 'O', ''),\n",
            " (last, 'B', 'TIME'),\n",
            " (24, 'I', 'TIME'),\n",
            " (hours, 'I', 'TIME'),\n",
            " (,, 'O', ''),\n",
            " (according, 'O', ''),\n",
            " (to, 'O', ''),\n",
            " (health, 'O', ''),\n",
            " (ministry, 'O', ''),\n",
            " (bulletin, 'O', ''),\n",
            " (on, 'O', ''),\n",
            " (Friday, 'B', 'DATE'),\n",
            " (., 'O', ''),\n",
            " (Active, 'O', ''),\n",
            " (caseload, 'O', ''),\n",
            " (stands, 'O', ''),\n",
            " (at, 'O', ''),\n",
            " (3,63,605, 'B', 'CARDINAL'),\n",
            " (;, 'O', ''),\n",
            " (lowest, 'O', ''),\n",
            " (in, 'O', ''),\n",
            " (150, 'B', 'DATE'),\n",
            " (days, 'I', 'DATE'),\n",
            " (., 'O', ''),\n",
            " (Meanwhile, 'O', ''),\n",
            " (,, 'O', ''),\n",
            " (the, 'O', ''),\n",
            " (recovery, 'O', ''),\n",
            " (rate, 'O', ''),\n",
            " (has, 'O', ''),\n",
            " (increased, 'O', ''),\n",
            " (to, 'O', ''),\n",
            " (97.54, 'B', 'PERCENT'),\n",
            " (%, 'I', 'PERCENT'),\n",
            " (., 'O', ''),\n",
            " (Stay, 'O', ''),\n",
            " (with, 'O', ''),\n",
            " (TOI, 'O', ''),\n",
            " (for, 'O', ''),\n",
            " (all, 'O', ''),\n",
            " (updates, 'O', '')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_58WCOLefyY"
      },
      "source": [
        "Lets take a few examples and analyse them with respect to the data.\n",
        "(i) United States - B - means it begins an entity of type GPE in the data.\n",
        "(ii) Billion - I - means it is inside an entity of type MONEY.\n",
        "(iii) the,and,to - more common words and are not recognisable entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "O6NYJnFRhvkj",
        "outputId": "56ffe6a0-693c-48f0-94d3-ca2c5c14ab3c"
      },
      "source": [
        "displacy.render(doc, jupyter=True, style=\"ent\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " reported \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    36,571\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " new \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Covid\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              " cases and \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    540\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " deaths in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    last 24 hours\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
              "</mark>\n",
              ", according to health ministry bulletin on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Friday\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". Active caseload stands at \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    3,63,605\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              "; lowest in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    150 days\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". Meanwhile, the recovery rate has increased to \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    97.54%\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
              "</mark>\n",
              ". Stay with TOI for all updates</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvZQ4lzL6i7T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}